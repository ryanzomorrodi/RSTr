---
title: "05: Calculating Parameter Medians"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{05: Calculating Parameter Medians}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

## Overview

In the previous vignette, we finally ran our model using `run_sampler()`, imported estimates using `load_samples()`, age-standardized and year-aggregated our estimates, and finally did some cursory exploration of large- and small-population counties in our dataset. Now, we can calculate our estimates, investigate measures of reliability, and investigate the implications of our measures of reliability. Note that this process is identical for all types of models.
## The `get_medians()` function

```{r, include = FALSE}
library(RSTr)
initialize_model(name = "my_test_model", data = miheart, adjacency = miadj)
run_sampler("my_test_model")
std_pop <- c(113154, 100640, 95799)
margin_age <- 2
theta <- load_samples("my_test_model") * 1e5
theta <- age_standardize(theta, std_pop, margin_age, groups = 1:3, bind_new = TRUE, new_name = "35-64")
```
In the `RSTr-samples` vignette, we generated age-standardized estimates for `theta` based on our example Michigan dataset. To get the medians, we simply load our samples into `get_medians()`:

```{r}
medians <- get_medians(theta)
```

From here, we can map our estimates:

```{r}
library(ggplot2)

est_35_64 <- medians[, "35-64", "1979"]

ggplot(mishp) +
  geom_sf(aes(fill = est_35_64)) +
  labs(
    title = "Smoothed Myocardial Infarction Death Rates in MI, Ages 35-64, 1979",
    fill = "Deaths per 100,000"
  ) +
  scale_fill_viridis_c() +
  theme_void()
```

Recall, though, in the previous vignette, we discovered that some traceplots were not as stable (i.e., reliable) as others. How can we test for reliability in our estimates?

## Estimate reliability

Reliability can be easily tested in CAR models using two criteria:

-   Relative precision: In [Quick, et al 2024](https://doi.org/10.1177/0282423X241244917), relative precision is measured as a ratio of the posterior median to its credible interval. If an estimate's ratio is less than 1, then that estimate is considered unreliable at that level of credibility. Effectively, this means that unreliable estimates have a spread of samples larger than the value of the estimate itself; and
-   Population counts: Though `RSTr` is designed to stabilize low-population regions, there is a limit to the amount of information the model can gather, and estimates from exceedingly low-population regions may be over-smoothed. Therefore, estimates from any region that fall below a specified threshold will be considered unreliable, regardless of relative precision. Use your judgment when setting this threshold depending on what kind of data you are working with: for example, 1,000 population is generally a good rule of thumb for mortality data, whereas an appropriate cutoff for birth data is closer to 100.

Let's get some reliability metrics for our dataset. First, let's generate our relative precisions at 95% credibility using the `get_relative_precision()` function and then create a `logical` `array` that tells us which estimates are unreliable:

```{r}
rel_prec <- get_relative_precision(sample = theta, medians = medians, perc_ci = 0.95)
low_rel_prec <- rel_prec < 1
```
