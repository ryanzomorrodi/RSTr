---
title: "06: Sample Processing"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{06: Sample Processing}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

## Overview

After initializing the model with `*car()`, you can gather and load samples using the `load_samples()` function. Finally, using some basic plotting, we can look at our results and make sure they make sense.

## The `run_sampler()` function

`run_sampler()` takes in four arguments: `name`, `dir`, `show_plots`, and `discard_burnin`. `name` and `dir` are associated with the name of the folder that contains the model content and the directory where that folder lives, respectively. `show_plots` allows you to hide the traceplots generated while the model runs and `discard_burnin` prevents samples from being saved before the 2000-iteration burn-in period in case the dataset is particularly large. The information needed to load the model is already saved in `name`, and so `run_sampler()` pulls this information in to generate parameter samples. When running `run_sampler()`, the R console will generate outputs describing the batch number, the total iteration number, and the time when the last batch started. By default, `run_sampler()` will generate 6000 samples, but other values can be specified as desired, rounded down to the nearest 100. For example, we can begin a simple model and generate samples:

```{r, results = "hide", fig.keep = "last"}
library(RSTr)
mod_mst <- mstcar(name = "my_test_model", data = miheart, adjacency = miadj)
```

## The `load_samples()` function

Once `run_sampler()` tells you that the model is finished running, you can import samples into R using `load_samples()`. `load_samples()` takes in four arguments:

-   `name`: Name of the model;

-   `dir`: Directory of the model;

-   `param`: The parameter to import samples for; and

-   `burn`: Specifies a burn-in period for samples. This allows the model time to stabilize before using samples to generate estimates. By default, has a burn-in period of 2000 samples.

Any `dimnames` that were saved to `data` will be applied to the samples as appropriate. Here, we pull in the `lambda` samples for our test Michigan dataset with a 2000-sample burn-in period (as specified by the default arguments). We also multiply by 100,000 as it is common to display mortality rates per 100,000 individuals:

```{r}
samples <- load_samples("my_test_model") * 1e5
```

### Group aggregation

In many cases, we will want to aggregate our data across non-age groups, such as when looking at prevalence estimates or to simply consolidate other sociodemographic groups. In our Michigan dataset, we have 10 years of data over which we can consolidate to look at prevalence. In these cases, we need to pull in the population array as a weight. First, we need to check which margin contains our year information using the `dim()` function:

```{r}
dim(samples)
```

Our `samples` array has four margins with dimensions `83`, `6`, `10`, and `400`, representing the spatial regions, age groups, time periods, and iterations, respectively. Let's set a variable `margin_time` to represent our time period margin and aggregate our `samples` estimates across 1979-1988 using the `aggregate_samples()` function. The population data needed to weight our samples can be pulled from the model folder using the `load_pop()` function:

```{r}
margin_time <- 3
pop <- load_pop("my_test_model")
samples_7988 <- aggregate_samples(samples, pop, margin_time)
```

Now, we have a standalone sample array for our 1979-1988 samples. But what if we are interested in both the individual year data *and* the prevalence data? We can alternatively bind these new samples to our main `samples` array by adding in values for the `bind_new` and `new_name` arguments:

```{r}
samples <- aggregate_samples(samples, pop, margin_time, bind_new = TRUE, new_name = "1979-1988")
```

### Age-standardization

The process of age-standardization is similar to that of group-aggregation, but requires a bit more nuance in its use. In our Michigan dataset, we have six 10-year age groups that start at age 35 years. We can age-standardize these into a new 35-64 group. Since we are using data from 1979-1988, we can use 1980 standard populations from [NIH](https://seer.cancer.gov/stdpopulations/stdpop.19ages.html) to generate a `std_pop` vector:

```{r}
age <- c("35-44", "45-54", "55-64")
std_pop <- c(113154, 100640, 95799)
names(std_pop) <- age
```

With `std_pop` generated, we need to then check which margin contains our age group information using the `dim()` function:

```{r}
dim(samples)
```

Our age groups lay along the second margin. Let's set a variable `margin_age` and standardize our `samples` estimates across ages 35-64 using the `age_standardize()` function:

```{r}
margin_age <- 2
samples_3564 <- age_standardize(samples, std_pop, margin_age, groups = c("35-44", "45-54", "55-64"))
```

Note that there may be times where you have groups stratified by both age and other sociodemographic groups. In these cases, you'll have to refactor your sample array so that the age groups are separated from your other groups before doing age-standardization, such as using `as.data.frame.table()` in conjunction with `xtabs()`.

Now, we have a standalone array for our age-standardized 35-64 age group. Similarly to `aggregate_samples()`, we can alternatively consolidate this into our main `samples` array by adding in values for the `bind_new` and `new_name` arguments:

```{r}
samples <- age_standardize(
    samples,
    std_pop,
    margin_age,
    groups = c("35-44", "45-54", "55-64"),
    bind_new = TRUE,
    new_name = "35-64"
)
```

Now, our samples for `samples` are aggregated by year and age-standardized, and we have matching array values for `pop`. Note that if you plan on doing a mix of non-age aggregation and age-standardization, do age-standardization *after* aggregation, as doing age-standardization first will alter the results of any aggregation done afterward.

## Closing Thoughts

In this vignette, we discussed generating samples with `run_sampler()`, importing those samples into R, age-standardization using `load_samples()`, generating traceplots to get a gut-check on our dataset, along with the implications of sporadic traceplots.